{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24324786-cd27-4dcb-9422-28514257c4f2",
   "metadata": {},
   "source": [
    "# Document Type Classification 2조\n",
    "\n",
    "### Contents\n",
    "- Import Library & Loading Data\n",
    "- EDA\n",
    "- Data Preprocessing\n",
    "- Define Functions\n",
    "- Modeling\n",
    "- Train Model\n",
    "- Result\n",
    "- Save File"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13cfd941-4711-4af7-aa43-6bb7dbbc7a33",
   "metadata": {},
   "source": [
    "## 1. Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "46cbc382-e839-4166-a891-2282a09d681a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import timm\n",
    "import torch\n",
    "import albumentations as A\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import albumentations as A\n",
    "import cv2\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.optim import Adam, AdamW\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import augraphy\n",
    "from sklearn.model_selection import KFold\n",
    "from glob import glob\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from timm import create_model\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a7166f1b-7783-4a30-92eb-766836ab76aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    model_name = \"resnet50\"\n",
    "    exp_name = f\"{model_name}-stepwise\"\n",
    "    img_size = 224\n",
    "    lr = 1e-3\n",
    "    step_size = 10  # Step size for LR scheduler\n",
    "    gamma = 0.1  # LR 감소율\n",
    "    epochs = 3\n",
    "    batch_size = 64\n",
    "    num_workers = 0\n",
    "    wd = 1e-4\n",
    "    alpha = 0.4\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    data_dir = \"/Users/choesumin/Documents/upstage/data\"\n",
    "    train_images_dir = os.path.join(data_dir, \"train\")\n",
    "    test_images_dir = os.path.join(data_dir, \"test\")\n",
    "    train_csv = os.path.join(data_dir, \"train.csv\")\n",
    "    meta_csv = os.path.join(data_dir, \"meta.csv\")\n",
    "    sample_submission_csv = os.path.join(data_dir, \"sample_submission.csv\")\n",
    "    best_model = os.path.join(data_dir, \"checkpoints\", f\"{model_name}_best_model.pth\")\n",
    "\n",
    "config = Config()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ded549-9b89-40f2-9715-50ed55009c36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f2104401-b797-4193-aa75-cf3caa2a6590",
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_classes(df, target_column=\"target\"):\n",
    "    class_counts = df[target_column].value_counts()\n",
    "    class_weights = 1. / class_counts\n",
    "    sample_weights = df[target_column].map(class_weights).values\n",
    "    return sample_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9b2360d3-161c-42fa-8021-248bfb9f1754",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixup_data(x, y, alpha=config.alpha):\n",
    "    lam = np.random.beta(alpha, alpha) if alpha > 0 else 1\n",
    "    batch_size = x.size(0)\n",
    "    index = torch.randperm(batch_size).to(x.device)\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed60829-b25b-4f43-a7b7-e2701598a7b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c0864864-bbec-4082-a680-6022a41c6ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augraphy 및 Albumentations 증강 적용\n",
    "trn_transform = A.Compose([\n",
    "    A.Resize(height=config.img_size, width=config.img_size),\n",
    "        A.OneOf([\n",
    "            A.GaussNoise(var_limit=(10.0, 800.0), p=0.75),\n",
    "            A.GaussianBlur(blur_limit=(1, 7), p=0.5)\n",
    "        ], p=0.75),\n",
    "        A.RandomRotate90(p=0.5),\n",
    "        A.HorizontalFlip(p=0.75),\n",
    "        A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2, p=0.5),\n",
    "        A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=30, p=0.25),\n",
    "        A.CoarseDropout(max_holes=6, max_height=32, max_width=32, p=0.5),\n",
    "        A.ElasticTransform(alpha=1, sigma=30, alpha_affine=30, p=0.5),\n",
    "        A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=0.5),\n",
    "        A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n",
    "        A.Rotate(limit=30, p=0.75),\n",
    "        A.CLAHE(clip_limit=2.0, tile_grid_size=(8, 8), p=0.5),\n",
    "        A.MotionBlur(blur_limit=5, p=0.5),\n",
    "        A.OpticalDistortion(p=0.5),\n",
    "        A.Transpose(p=0.5),\n",
    "        A.Normalize(mean=[0.5743355787358306, 0.583304060105453, 0.588189268004516], std=[0.18964056010820557, 0.18694252072057746, 0.1850691924647016]),\n",
    "        ToTensorV2(),\n",
    "])\n",
    "\n",
    "tst_transform = A.Compose([\n",
    "    A.Resize(config.img_size, config.img_size),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95196ddb-8192-4be2-b497-7857b06c8765",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "32aaa119-4013-4e9b-bda9-4d71c708c176",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, df, path, transform=None):\n",
    "        self.df = df.values\n",
    "        self.path = path\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        name, target = self.df[idx]\n",
    "        img = np.array(Image.open(os.path.join(self.path, name)))\n",
    "        if self.transform:\n",
    "            img = self.transform(image=img)[\"image\"]\n",
    "        return img, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e0048d-5174-4acd-8783-e6a2ce79dd1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b9db5960-cf02-4fbc-88b1-fdf411cb3b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습 함수\n",
    "def train(loader, model, optimizer, loss_fn, device):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    preds_list = []\n",
    "    targets_list = []\n",
    "\n",
    "    pbar = tqdm(loader)\n",
    "    for image, targets in pbar:\n",
    "        image = image.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        model.zero_grad(set_to_none=True)\n",
    "\n",
    "        preds = model(image)\n",
    "        loss = loss_fn(preds, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n",
    "        targets_list.extend(targets.detach().cpu().numpy())\n",
    "\n",
    "        pbar.set_description(f\"Train Loss: {loss.item():.4f}\")\n",
    "\n",
    "    train_loss /= len(loader)\n",
    "    train_acc = accuracy_score(targets_list, preds_list)\n",
    "    train_f1 = f1_score(targets_list, preds_list, average=\"macro\")\n",
    "\n",
    "    return train_loss, train_acc, train_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ae3eb7-621d-45d0-875e-c4fd5a51dbd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f0167a27-2131-458a-93af-1432903a2add",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 검증 함수\n",
    "def validate(loader, model, loss_fn, device):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    preds_list, targets_list = [], []\n",
    "    \n",
    "    wrong_images, wrong_preds, wrong_labels = [], [], []\n",
    "    total_preds, total_targets = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(loader)\n",
    "        for image, targets in pbar:\n",
    "            image = image.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            preds = model(image)\n",
    "            loss = loss_fn(preds, targets)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            preds_np = preds.argmax(dim=1).cpu().numpy()\n",
    "            targets_np = targets.cpu().numpy()\n",
    "            for i in range(len(preds_np)):\n",
    "                total_preds.append(preds_np[i])\n",
    "                total_targets.append(targets_np[i])\n",
    "                if preds_np[i] != targets_np[i]:\n",
    "                    wrong_images.append(image[i].cpu().numpy().transpose(1, 2, 0))\n",
    "                    wrong_preds.append(preds_np[i])\n",
    "                    wrong_labels.append(targets_np[i])\n",
    "\n",
    "            preds_list.extend(preds_np)\n",
    "            targets_list.extend(targets_np)\n",
    "            pbar.set_description(f\"Valid Loss: {loss.item():.4f}\")\n",
    "\n",
    "    val_loss /= len(loader)\n",
    "    val_acc = accuracy_score(targets_list, preds_list)\n",
    "    val_f1 = f1_score(targets_list, preds_list, average=\"macro\")\n",
    "\n",
    "    return val_loss, val_acc, val_f1, wrong_images, wrong_preds, wrong_labels, total_preds, total_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "274cdd3e-d896-472d-a2d9-e203da62e456",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(loader, model, device):\n",
    "    test_preds_list = []\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for image, _ in tqdm(loader, desc=\"Testing\"):\n",
    "            image = image.to(device)\n",
    "\n",
    "            preds = model(image)\n",
    "            test_preds_list.extend(preds.argmax(dim=1).cpu().numpy())\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    test_df = pd.DataFrame(tst_dataset.df, columns=[\"ID\", \"target\"])\n",
    "    test_df[\"target\"] = test_preds_list\n",
    "\n",
    "    sample_submission_df = pd.read_csv(os.path.join(config.data_dir, \"sample_submission.csv\"))\n",
    "    if not (sample_submission_df[\"ID\"] == test_df[\"ID\"]).all():\n",
    "        raise ValueError(\"Mismatch in IDs between submission and test predictions.\")\n",
    "\n",
    "    test_df.to_csv(config.submission_output_csv, index=False)\n",
    "\n",
    "    print(\"Test predictions saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "382af6ef-a5de-41c2-a025-9776657d9e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bb91d18e-9d4f-4b5e-be65-7f4cc4dd0f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(os.path.join(config.data_dir, \"train.csv\"))\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=SEED, stratify=train_df[\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ce6797-a4bc-4b4b-9dd0-98fb31d1bb19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ea585b-faeb-4e81-bf74-45a328080c1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c23747-15ef-4c0c-9b28-d9d9aa2d86ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e1538b3f-9dba-467b-8e25-f0441ba4687c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.8413: 100%|███████████████████████| 25/25 [07:36<00:00, 18.27s/it]\n",
      "Valid Loss: 2.8222: 100%|█████████████████████████| 5/5 [00:39<00:00,  7.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "Train Loss: 2.8332, Train Acc: 0.0720, Train F1: 0.0254\n",
      "Val Loss: 2.8154, Val Acc: 0.1051, Val F1: 0.0372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.8456: 100%|███████████████████████| 25/25 [07:16<00:00, 17.47s/it]\n",
      "Valid Loss: 2.8091: 100%|█████████████████████████| 5/5 [00:38<00:00,  7.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3\n",
      "Train Loss: 2.8230, Train Acc: 0.0624, Train F1: 0.0327\n",
      "Val Loss: 2.8030, Val Acc: 0.0924, Val F1: 0.0345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.8077: 100%|███████████████████████| 25/25 [07:31<00:00, 18.04s/it]\n",
      "Valid Loss: 2.7955: 100%|█████████████████████████| 5/5 [00:39<00:00,  7.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3\n",
      "Train Loss: 2.8076, Train Acc: 0.1057, Train F1: 0.0797\n",
      "Val Loss: 2.7918, Val Acc: 0.1146, Val F1: 0.0609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 모델 및 학습 초기화\n",
    "if __name__ == \"__main__\":\n",
    "    model = timm.create_model(config.model_name, pretrained=True, num_classes=17).to(config.device)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=config.lr, momentum=0.9, weight_decay=config.wd)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=config.step_size, gamma=config.gamma)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    # 데이터 불균형 해결을 위한 샘플링 가중치 계산\n",
    "    train_df = pd.read_csv(config.train_csv)\n",
    "    sample_weights = balance_classes(train_df)\n",
    "    sampler = torch.utils.data.WeightedRandomSampler(weights=sample_weights, num_samples=len(sample_weights))\n",
    "\n",
    "    # 데이터 로더\n",
    "    trn_loader = DataLoader(ImageDataset(train_df, config.train_images_dir, trn_transform),\n",
    "                            batch_size=config.batch_size, sampler=sampler, num_workers=config.num_workers)\n",
    "    val_df = train_test_split(train_df, test_size=0.2, stratify=train_df[\"target\"])[1]\n",
    "    val_loader = DataLoader(ImageDataset(val_df, config.train_images_dir, tst_transform),\n",
    "                            batch_size=config.batch_size, shuffle=False, num_workers=config.num_workers)\n",
    "\n",
    "    for epoch in range(3):\n",
    "        train_loss, train_acc, train_f1 = train(trn_loader, model, optimizer, loss_fn, config.device)\n",
    "        val_loss, val_acc, val_f1, _, _, _, _, _ = validate(val_loader, model, loss_fn, config.device)  # 필요한 값만 변수에 저장\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{config.epochs}\")\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Train F1: {train_f1:.4f}\")\n",
    "        print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, Val F1: {val_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c48b94-e0dc-4e5d-beaa-51643302d851",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa16157d-4f0b-4ba7-be04-f9dd6d6bc401",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50498db9-7894-4d22-8fc4-f4672730eaa7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
